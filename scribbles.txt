2024-07-19 | poetry stuff
https://www.digitalocean.com/community/tutorials/how-to-publish-python-packages-to-pypi-using-poetry-on-ubuntu-22-04

$ poetry config pypi-token.pypi your-api-token
$ poetry build
$ poetry publish

poetry show
poetry shell
poetry env remove python
poetry install
poetry lock
poetry add uvicorn

(could not get it to work with uvicorn, but this (pyproject.toml) should work:
[tool.poetry.scripts]
start = "uvicorn purr_geographix.main:app --reload"
start-prod = "uvicorn purr_geographix.main:app --workers 4"
Then: poetry run start-prod

Used this instead:
poetry run uvicorn purr_geographix.main:app --reload


2024-07-18 | snippet for timing network scan
# async def main():
#     import time
#
#     t0 = time.time()
#     recon_root = r"d:/"
#
#     repos = await network_repo_scan(recon_root)
#     t1 = time.time()
#
#     for r in repos:
#         print(r)
#     print(t1 - t0)
#
#
# if __name__ == "__main__":
#     asyncio.run(main())



2024-07-18 | removed handling lists of SQL to simplify things
# @retry(RetryException, tries=5)
# def db_exec(
#     conn: Union[dict, "SQLAnywhereConn"], sql: Union[str, List[str]]
# ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:
#     """
#     Args:
#         conn (Union[dict, 'SQLAnywhereConn']):
#
#     Returns:
#         Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:
#     """
#
#     # if isinstance(conn, SQLAnywhereConn):
#     #     conn = conn.to_dict()
#
#     try:
#         with pyodbc.connect(**conn) as connection:
#             with connection.cursor() as cursor:
#                 if isinstance(sql, str):
#                     cursor.execute(sql)
#
#                     return [
#                         dict(zip([col[0] for col in cursor.description], row))
#                         for row in cursor.fetchall()
#                     ]
#
#                 if isinstance(sql, list):
#                     results = []
#                     for s in sql:
#                         cursor.execute(s)
#                         results.append(
#                             [
#                                 dict(zip([col[0] for col in cursor.description], row))
#                                 for row in cursor.fetchall()
#                             ]
#                         )
#                     return results
#
#     except pyodbc.OperationalError as oe:
#         if re.search(r"Database name not unique", str(oe)):
#             conn.pop("dbf")
#             raise RetryException from oe
#     except pyodbc.ProgrammingError as pe:
#         if re.search(r"Table .* not found", str(pe)):
#             return pe
#
#     except Exception as ex:
#         raise ex

used like this:

    # res = db_exec(repo_base["conn"], list(counter_sql.values()))
    #
    # counts = {}
    #
    # if isinstance(res, Exception):
    #     logger.error({"context": repo_base, "error": res})
    #     for i, k in enumerate(counter_sql.keys()):
    #         counts[k] = None
    #     return counts
    #
    # for i, k in enumerate(counter_sql.keys()):
    #     counts[k] = res[i][0]["tally"] or 0



2024-07-12 | maybe send streaming response
If some future version opts to send a JSON response, use CustomJSONResponse:
    class CustomJSONResponse(JSONResponse):
        def render(self, content: any) -> bytes:
            return json.dumps(
                content,
                ensure_ascii=False,
                allow_nan=False,
                indent=None,
                separators=(",", ":"),
                cls=CustomEncoder,
            ).encode("utf-8")

Maybe a streaming response makes more sense?
https://stackoverflow.com/questions/75740652/fastapi-streamingresponse-not-streaming



2024-07-11 | xformers from purrio_geographix
No longer needed?:
blob_to_hex: covered by formatter
delimited_array_with_nulls: pandas
decode_depth_registration: raster_log, we skip log_depth_cal_vec, not needed
decode_curve_values: vector_log, dupicated, but I suppose we could.

    if func_name == "blob_to_hex":
        return row[col].hex()

    if func_name == "delimited_array_with_nulls":
        values = row[col].split(purr_delimiter)
        return [ensure_type(data_type, v) if v != purr_null else None for v in values]

    if func_name == "decode_depth_registration":
        reg_points = []
        buf = bytearray(row[col])
        for i in range(12, len(buf), 28):
            depth_bytes = buf[i : i + 8]  # 64-bit float (double)
            depth = struct.unpack("d", depth_bytes)[0]
            pixel_bytes = buf[i + 12 : i + 16]  # 32-bit integer
            pixel = struct.unpack("i", pixel_bytes)[0]
            reg_points.append({"depth": depth, "pixel": pixel})
        return reg_points

    if func_name == "decode_curve_values":
        curve_vals = []
        buf = bytearray(row[col])
        for i in range(2, len(buf), 4):
            cval_bytes = buf[i : i + 4]  # 32 bit float
            cval = struct.unpack("<f", cval_bytes)[0]
            curve_vals.append(cval)
        return curve_vals
    else:
        if data_type not in ("object", "string", "number", "date"):
            print("--------NEED TO ADD XFORM-------->", data_type)
        return ensure_type(data_type, row[col])



# yanked out 2024-07-07

class SettingsWithRepos(BaseModel):
    settings: Optional[Settings] = None
    repos: List[Repo] = []

@router.get("/settings/", response_model=schemas.SettingsWithRepos)
def read_settings(db: Session = Depends(get_db)):
    result = crud.get_settings(db)
    return schemas.SettingsWithRepos(
        settings=result["settings"] if result["settings"] else None,
        repos=result["repos"],
    )


# THIS WORKS as of 2024-06-16
# def upsert_settings(db: Session, settings: schemas.SettingsCreate):
#     stmt = text("""
#         INSERT INTO settings (file_depot)
#         VALUES (:file_depot)
#         ON CONFLICT(file_depot) DO UPDATE SET file_depot = excluded.file_depot
#         """)
#     db.execute(stmt, {"file_depot": settings.file_depot})
#     db.commit()
#
#     result = db.execute(text("SELECT file_depot FROM settings")).fetchone()
#     return schemas.Settings(file_depot=result[0] if result else None)

# works for settings everything at once
# def write_settings(db: Session, settings: schemas.SettingsCreate):
#     delete_stmt = text("DELETE FROM settings")
#     db.execute(delete_stmt)
#     insert_stmt = text("INSERT INTO settings (file_depot) VALUES (:file_depot)")
#     db.execute(insert_stmt, {"file_depot": settings.file_depot})
#     db.commit()
#     result = db.execute(text("SELECT file_depot FROM settings")).fetchone()
#     return schemas.Settings(file_depot=result[0] if result else None)

# class CustomJSONResponse(JSONResponse):
#     def render(self, content: any) -> bytes:
#         return json.dumps(
#             content,
#             ensure_ascii=False,
#             allow_nan=False,
#             indent=None,
#             separators=(",", ":"),
#             cls=CustomEncoder,
#         ).encode("utf-8")
